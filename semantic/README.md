# Textual Semantic Similarity Code Description
## 1. Installation

First, install the necessary python libraries:

```shell
pip install -U FlagEmbedding
pip install transformers==4.34.0
```

Download the Embedding model:

Note that the link provided is for the **Chinese version** of the model, you can also choose the English version of the model on your own.

```shell
mkdir -p models
cd models
git clone https://huggingface.co/BAAI/bge-large-zh-v1.5/tree/main
cd ..
```

## 2. Usage

Create the Muchindata_Analyzer class and import the label data file:

```python
Muer = MuChindata_Analyzer('./data/labels-free.xlsx', './models/bge-large-zh-v1.5')
```

In particular, the names of the columns in the `./data/labels-free.xlsx` file, the names of the columns and examples of the columns are as follows (in English):

- Song Title : `2dc87b134a51446087be217e3bb066e3`
- Feeling:  `This song belongs to the lyrical song, the song expresses the gratitude to the ...`
- Style or Genre: `Popular Song, Lyric Song, Good Brother`
- Expressive Feeling (Singer and Accompaniment):`Very emotional, Cosmic, Cheerful, Happy, Tearful`
- Emotional Feeling (Lyrics and Melody): `Heartfelt, Realistic, Expressive, Companion, Envious`
- Tempo and Rhythm: `Faster, Soothing`
- Lyric Themes: `Directness, Expression, Appreciation`
- Instrumentation and Audio Effects: `Electric Guitar, Bass, Drum Kit, Piano`
- Vocals Components: `Male, Solo, Backing Vocals`
- Sound Quality: `Clear`
- Live: `No`
- Lo-Fi: `No`
- Song Purpose: `Expression, Brotherhood`
- Culture and Region: `No regional cultural characteristics`
- Target Audience: `middle-aged people, young people`

An example of `./data/labels-free.xlsx` is provided.

Calculate the semantic similarity of two collections of text labels

```python
list_1 = [["Nostalgia", "Pop Songs", "Fleeting"], 
          ["Slow", "Medium", "Slower"], 
          ["Emotional Application", "Narration", "Sothing", "Sentimental Nostalgia"], 
          ["Nostalgic", "Serene", "Melancholic", "Others"], 
          ["Western Instruments", "Vocal and Performance", "Ethnic and Traditional Instruments"], 
          ["Harmony", "Voice Characteristics"], 
          ["Clear"], 
          ["Sleep Aids"], 
          ["No Regional or Cultural Characteristics"], 
          ["Adults", "Children and Adolescents"]]
list_2 = [["Nostalgia", "Style and Genre", "Pop Songs", "Fleeting"], 
          ["Slow", "Medium", "Slower"], 
          ["Emotional Application", "Narration", "Sothing", "Sentimental Nostalgia"], 
          ["Nostalgic", "Joy", "Serene", "Others"], 
          ["Western Instruments", "Vocal and Performance", "Ethnic and Traditional Instruments"], 
          ["Harmony", "Voice Characteristics"], 
          ["Clear"], 
          ["Sleep Aids"], 
          ["No Regional or Cultural Characteristics"], 
          ["Adults"]]
res = Muer.cal_word_similarity(list_1, list_2)
```

Based on the above semantic similarity comparison code, we compared the differences between professional and amateur annotators:

```python
# When cmp_kinds are the same, compare the self_kinds labeling data of professional and amateur annotators
cmp_kinds = ['Style or Genre','Song Purpose', 'Culture and Region', 'Target Audience']
self_kinds_amateur = ['Emotional Feeling (Lyrics and Melody)']
Muer.solve_labels(cmp_kinds, self_kinds_amateur)
```

Further, we also evaluated the effectiveness of the understanding model:

```python
Muer.compmodel_eval()
```

This function extracts the text generated by the understanding model and compares the annotations provided by the annotator. The results of the comparison are stored in the corresponding `.npy` file. 

For example, if we want to test the effect of `jukebox` model, then please save the results of GroundTruth with `jukebox` model as a list in `data/cpmodel_eval/jukebox-output.txt`, and after the function has run, the results will be saved as `exp/cpeval _bf/jukebox-output.npy`.

An example of model output is provided as `data/cpmodel_eval/jukebox-output.txt`.